import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm 
### BUGï¼šæ··åˆäº†Tensorflow keraså’Œkeras APIã€‚ä¼˜åŒ–å™¨å’Œæ¨¡å‹åº”æ¥è‡ªåŒä¸€å±‚å®šä¹‰ã€‚
from scipy.signal import resample
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras import Input, Model, regularizers
from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU
# tensorflow 2.0æ²¡æœ‰åŠ å…¥å¯¹CuDNNLSTMçš„æ”¯æŒã€‚ä½†æ˜¯ï¼Œtensorflowçš„LSTMå¯¹GPUåŠ é€Ÿä¼˜åŒ–çš„ç¨€çƒ‚ï¼Œä¸“é—¨å¼•å…¥åªèƒ½ç”±GPUåŠ é€Ÿå¹¶è¡Œè¿ç®—çš„CuDNNLSSTMï¼ˆï¼‰
# æŠŠLSTMæ”¹æˆCuDNNLSTMä¹‹åï¼Œè®­ç»ƒé€Ÿåº¦è‡³å°‘æå‡äº†5å€ä»¥ä¸Š
# CuDNNLSTMæ˜¯ä¸ºCUDAå¹¶è¡Œå¤„ç†è€Œè®¾è®¡çš„ï¼Œå¦‚æœæ²¡æœ‰GPUï¼Œå®ƒå°†æ— æ³•è¿è¡Œã€‚è€ŒLSTMæ˜¯ä¸ºæ™®é€šCPUè®¾è®¡çš„ã€‚ç”±äºå¹¶è¡Œæ€§ï¼Œæ‰§è¡Œæ—¶é—´æ›´å¿«ã€‚
from tensorflow.keras.layers import GaussianNoise, LSTM, GRU, Bidirectional, Layer, Conv1D, BatchNormalization, MaxPooling1D, AveragePooling1D, Dropout, SpatialDropout1D, Dense, concatenate, Activation, Lambda, dot
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
### æ³¨æ„ï¼šä¸ºäº†é˜²æ­¢ä¸å¿…è¦çš„BUGï¼Œç”¨kerasçš„æ—¶å€™ï¼Œå¿…é¡»è¦å¸¦ä¸Štensorflowï¼Œå³tensorflow.kerasï¼Œé‡åˆ°çš„BUGï¼šå…ƒç»„å¯¹è±¡æ²¡æœ‰layerå±æ€§ï¼Œ
### å¦å¤–ï¼Œåªæœ‰keraså¯èƒ½å¯¼è‡´è°ƒç”¨çš„ä¸œè¥¿æ— æ•ˆã€‚æ¯”å¦‚ï¼ŒReduceLROnPlateauåœ¨ç”¨keras.callbacksä¸­importçš„æ—¶å€™ï¼Œæ²¡æœ‰èµ·ä½œç”¨ï¼Œä»tensorflow.keras.callbacksè°ƒç”¨æœ‰ä½œç”¨ã€‚
from sklearn.model_selection import StratifiedKFold

### æ‰§è¡Œä¸‹é¢è¯­å¥èƒ½å¤Ÿå°½é‡å‡å°‘æ¯æ¬¡kerasåˆ†æ•°çš„ä¸ç¡®å®š ###
SEED = 42
import random
import os
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['PYTHONHASHSEED'] = str(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)
######################################################

datasets_path = './data/'
models_path = './model/'
sub_path = '/content/drive/My Drive/kesci_JZB/Baseline/sub/'

def acc_combo(y, y_pred, mode):
    # æ•°å€¼IDä¸è¡Œä¸ºç¼–ç çš„å¯¹åº”å…³ç³»
  mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', 4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', 
          8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', 12: 'C_1', 13: 'C_3', 14: 'C_0', # é€’æ‰‹æœº
          15: 'B_6', 16: 'C_2', 17: 'C_5', 18: 'C_6' }
  if mode == 'behavior':  # åœºæ™¯+åŠ¨ä½œ
    code_y, code_y_pred = mapping[y], mapping[y_pred] 
    if code_y == code_y_pred: # ç¼–ç å®Œå…¨ç›¸åŒå¾—åˆ†1.0 å³ C_0 == C_0
      return 1.0
    elif code_y.split("_")[0] == code_y_pred.split("_")[0]: # åœºæ™¯ç›¸åŒå¾— 1.0/7 åˆ†
      return 1.0/7
    elif code_y.split("_")[1] == code_y_pred.split("_")[1]: # åŠ¨ä½œç›¸åŒå¾— 1.0/3 åˆ†
      return 1.0/3
    else: # éƒ½ä¸å¯¹ï¼Œä¸å¾—åˆ†
      return 0.0 

  # if mode == 'scene':  # æœ€é«˜å¾—åˆ°7500/7 = 1071åˆ†  0.78å·¦å³
  #     mapping_scene = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}
  #     code_y, code_y_pred = mapping_scene[y], mapping_scene[y_pred]
  #     if code_y == code_y_pred:
  #         return 1.0/7
  #     else: 
  #         return 0.0

  # if mode == 'action':  # æœ€é«˜å¾—åˆ°7500/3 = 2500åˆ†
  #     mapping_action = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6'}
  #     code_y, code_y_pred = mapping_action[y], mapping_action[y_pred]

  #     if code_y == code_y_pred: 
  #         return 1.0/3
  #     else:
  #         return 0.0




train = pd.read_csv(datasets_path+'sensor_train.csv')
test = pd.read_csv(datasets_path+'sensor_test.csv')
sub = pd.read_csv(datasets_path+'result.csv')
# train['train_scene'] = train['behavior_id'].apply(lambda x: 0 if x==0 or x==1 or x==2 or x==3 or x==5 or x==11 else 
#                                 1 if x==6 or x==7 or x==8 or x==9 or x==10 or x==15 else 
#                                 2 if x==12 or x==13 or x==14 or x==16 or x==17 or x==18 else 3)

# train['train_action'] = train['behavior_id'].apply(lambda x: 0 if x==0 or x==10 or x==14 else 1 if x==1 or x==6 or x==12 else 
#                                  2 if x==2 or x==8 or x==6 else 3 if x==3 or x==9 or x==13 else 
#                                  6 if x==11 or x==15 or x==18 else 5 if x==5 or x==7 or x==17 else 4)

# y_scene = train.groupby('fragment_id')['train_scene'].min()
# y_action = train.groupby('fragment_id')['train_action'].min()
y = train.groupby('fragment_id')['behavior_id'].min()

## æ±‚åŠ é€Ÿåº¦çš„æ¨¡
train['mod'] = (train.acc_x ** 2 + train.acc_y ** 2 + train.acc_z ** 2) ** .5
train['modg'] = (train.acc_xg ** 2 + train.acc_yg ** 2 + train.acc_zg ** 2) ** .5
test['mod'] = (test.acc_x ** 2 + test.acc_y ** 2 + test.acc_z ** 2) ** .5
test['modg'] = (test.acc_xg ** 2 + test.acc_yg ** 2 + test.acc_zg ** 2) ** .5

## 8ä¸ªåˆ†åˆ«ä¸€é˜¶å·®åˆ†
train_diff1 = pd.DataFrame()
diff_fea = ['acc_x','acc_y','acc_z','acc_xg','acc_yg','acc_zg','mod','modg']
train_diff1 = train.groupby('fragment_id')[diff_fea].diff(1).fillna(0.) 
train_diff1.columns = ['x_diff_1','y_diff_1','z_diff_1','xg_diff_1','yg_diff_1','zg_diff_1','mod_diff_1','modg_diff_1']

test_diff1 = pd.DataFrame()
test_diff1 = test.groupby('fragment_id')[diff_fea].diff(1).fillna(0.)
test_diff1.columns = train_diff1.columns
## 8ä¸ªåˆ†åˆ«äºŒé˜¶å·®åˆ†
train_diff2 = pd.DataFrame()
train_diff2 = train.groupby('fragment_id')[diff_fea].diff(2).fillna(0.) 
train_diff2.columns = ['x_diff_2','y_diff_2','z_diff_2','xg_diff_2','yg_diff_2','zg_diff_2','mod_diff_2','modg_diff_2']

test_diff2 = pd.DataFrame()
test_diff2 = test.groupby('fragment_id')[diff_fea].diff(2).fillna(0.)
test_diff2.columns = train_diff2.columns

## èåˆ
train = pd.concat([train, train_diff1, train_diff2], axis = 1)
test = pd.concat([test, test_diff1, test_diff2], axis = 1)

No_train_fea = ['fragment_id', 'time_point', 'behavior_id', 'train_scene', 'train_action']
train_fea = [fea for fea in train.columns if fea not in No_train_fea]
fea_num = len(train_fea)
## å½’ä¸€åŒ–
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
train[train_fea] = pd.DataFrame(scaler.fit_transform(train[train_fea]), columns = train_fea)
test[train_fea] = pd.DataFrame(scaler.fit_transform(test[train_fea]), columns = train_fea)
'''
åˆ†æï¼š
å¦‚æœæŸä¸ªç‰¹å¾çš„æ–¹å·®è¿œå¤§äºå…¶å®ƒç‰¹å¾çš„æ–¹å·®ï¼Œé‚£ä¹ˆå®ƒå°†ä¼šåœ¨ç®—æ³•å­¦ä¹ ä¸­å æ®ä¸»å¯¼ä½ç½®ï¼Œå¯¼è‡´æˆ‘ä»¬çš„å­¦ä¹ å™¨ä¸èƒ½åƒæˆ‘ä»¬æœŸæœ›çš„é‚£æ ·ï¼Œå»å­¦ä¹ å…¶ä»–çš„ç‰¹å¾ï¼Œ
è¿™å°†å¯¼è‡´æœ€åçš„æ¨¡å‹æ”¶æ•›é€Ÿåº¦æ…¢ç”šè‡³ä¸æ”¶æ•›ï¼Œå› æ­¤éœ€è¦å¯¹è¿™æ ·çš„ç‰¹å¾æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–/å½’ä¸€åŒ–ã€‚è½¬åŒ–å‡½æ•°ä¸ºï¼šx =(x - ğœ‡)/ğœ
'''



## åŸ
x = np.zeros((7292, 60, fea_num)) 
t = np.zeros((7500, 60, fea_num))
## å…ˆé‡‡é›†åŸæ¥çš„æ•°æ®é›†
for i in tqdm(range(7292)):
  tmp = train[train.fragment_id == i][:60].reset_index(drop = True) 
  x[i, :, :] = resample(tmp[train_fea], 60, np.array(tmp.time_point))[0]
for i in tqdm(range(7500)):
  tmp = test[test.fragment_id == i][:60].reset_index(drop = True)
  t[i, :, :] = resample(tmp[train_fea], 60, np.array(tmp.time_point))[0]


# è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°ï¼šget_acc_combo()
def get_acc_combo():
    def combo(y, y_pred):
        # æ•°å€¼IDä¸è¡Œä¸ºç¼–ç çš„å¯¹åº”å…³ç³»
        mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3',
                4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5',
                8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6',
                12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6',
                16: 'C_2', 17: 'C_5', 18: 'C_6'}
        # å°†è¡Œä¸ºIDè½¬ä¸ºç¼–ç 

        code_y, code_y_pred = mapping[int(y)], mapping[int(y_pred)]
        if code_y == code_y_pred: #ç¼–ç å®Œå…¨ç›¸åŒå¾—åˆ†1.0
            return 1.0
        elif code_y.split("_")[0] == code_y_pred.split("_")[0]: #ç¼–ç ä»…å­—æ¯éƒ¨åˆ†ç›¸åŒå¾—åˆ†1.0/7
            return 1.0/7
        elif code_y.split("_")[1] == code_y_pred.split("_")[1]: #ç¼–ç ä»…æ•°å­—éƒ¨åˆ†ç›¸åŒå¾—åˆ†1.0/3
            return 1.0/3
        else:
            return 0.0
    confusionMatrix=np.zeros((19,19))
    for i in range(19):
      for j in range(19):
        confusionMatrix[i,j]=combo(i,j)
    confusionMatrix=tf.convert_to_tensor(confusionMatrix)

    def acc_combo(y, y_pred):
      y=tf.argmax(y,axis=1)
      y_pred = tf.argmax(y_pred, axis=1)
      indices=tf.stack([y,y_pred],axis=1)
      scores=tf.gather_nd(confusionMatrix,tf.cast(indices,tf.int32))
      return tf.reduce_mean(scores)
    return acc_combo



epochs = 1
batch_size = 120
# å°è¯•ï¼šä»¥128ä¸ºåˆ†ç•Œçº¿ï¼Œå‘ä¸‹ï¼ˆ*0.5ï¼‰å’Œå‘ä¸Šï¼ˆ*2ï¼‰è®­ç»ƒåæ¯”è¾ƒæµ‹è¯•ç»“æœï¼Œè‹¥å‘ä¸‹æ›´å¥½åˆ™å†*0.5ï¼Œç›´æ¥ç»“æœä¸å†æå‡
# batchsizeè®¾ç½®ï¼šé€šå¸¸10åˆ°100ï¼Œä¸€èˆ¬è®¾ç½®ä¸º2çš„næ¬¡æ–¹ã€‚åŸå› ï¼šè®¡ç®—æœºçš„gpuå’Œcpuçš„memoryéƒ½æ˜¯2è¿›åˆ¶æ–¹å¼å­˜å‚¨çš„ï¼Œè®¾ç½®2çš„næ¬¡æ–¹å¯ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦ã€‚
kernel_size = 3 
pool_size = 2
dropout_rate = 0.4 # é˜²æ­¢è¿‡æ‹Ÿåˆ
n_classes = 19
# n_action = 7
# n_scene = 3
act_swish = lambda x:x * tf.nn.sigmoid(x)
# Swish æ˜¯ä¸€ç§æ–°å‹æ¿€æ´»å‡½æ•°ï¼Œå…¬å¼ä¸ºï¼š f(x) = x Â· sigmoid(x)ã€‚Swish å…·å¤‡æ— ä¸Šç•Œæœ‰ä¸‹ç•Œã€å¹³æ»‘ã€éå•è°ƒçš„ç‰¹æ€§
proba_tA = np.zeros((7500, n_classes)) 
valid = np.zeros((7292, n_classes))
y_ = to_categorical(y, n_classes) ### Qï¼šè®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹çš„åˆ†æ•°ç‰¹åˆ«ä½ Aï¼šè¿™é‡Œçš„yç”¨çš„æ˜¯y_sceneï¼ˆåœºæ™¯æ ‡ç­¾ï¼‰è€Œä¸æ˜¯yï¼ˆè¡Œä¸ºæ ‡ç­¾ï¼‰
## losså¹³æ»‘æ ‡ç­¾
# CC_Ls = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.1) åŠ äº†æ ‡ç­¾å¹³æ»‘åˆ†æ•°ä¸‹é™äº†

# xyzã€modã€modg ä¸€ã€äºŒé˜¶å·®åˆ† + ä¸‰å±‚å•å‘LSTM + StandardScaler çº¿ä¸‹ 0.82134 çº¿ä¸Š 0.7437936507936508
def Net(): 
  input = Input(shape=(60, fea_num))
  # model = GaussianNoise(0.1)(input) # å…ˆç»™æ•°æ®åŠ å…¥é«˜æ–¯å™ªå£°è¿›è¡Œæ•°æ®å¢å¼ºï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ  ä¸ºæ•°æ®æ–½åŠ 0å‡å€¼ï¼Œæ ‡å‡†å·®ä¸ºstddevçš„åŠ æ€§é«˜æ–¯å™ªå£°
  model = Conv1D(1024, kernel_size, input_shape=(60, fea_num), activation=act_swish, padding='same', kernel_regularizer = regularizers.l2(0.01))(input) 
  model = BatchNormalization()(model) 
  model = AveragePooling1D(pool_size=pool_size)(model) 
  model = Dropout(dropout_rate)(model) # ä¸€èˆ¬æ¥è¯´ï¼ŒDropoutä»…åœ¨æ± åŒ–å±‚åä½¿ç”¨
  
  model = Conv1D(512, kernel_size, activation=act_swish, padding='same')(model) 
  model = BatchNormalization()(model) 
  model = AveragePooling1D(pool_size=pool_size)(model)  
  model = Dropout(dropout_rate)(model)

  model = Conv1D(256, kernel_size, activation=act_swish, padding='same')(model) 
  model = BatchNormalization()(model)
  model = AveragePooling1D(pool_size=pool_size)(model) 

  # å•å‘lstm
  model = CuDNNLSTM(180, return_sequences=True)(model) # GRUæ¯”LSTMå°‘ä¸€ä¸ªé—¨ï¼Œè®­ç»ƒçš„å‚æ•°å°‘äº†ï¼Œå®¹æ˜“è®­ç»ƒä¸”å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ CuDNNGRU  CuDNNLSTM
  model = CuDNNLSTM(180, return_sequences=True)(model) 
  model = CuDNNLSTM(180)(model)
  # model = CuDNNGRU(150, return_sequences=True)(model) # (None,180)
  # model = attention_3d_block(model)


  ## åŒå‘lstm + attention
  # model = Bidirectional(LSTM(180, return_sequences=True))(model)  # é»˜è®¤æ¿€æ´»å‡½æ•°ä¸ºtanh
  # model = Bidirectional(LSTM(180, return_sequences=True))(model)
  # model = Bidirectional(LSTM(180, return_sequences=True))(model)
  # model = attention_3d_block(model)

  model = Dropout(dropout_rate)(model) 
  model = Dense(n_classes)(model) 
  model = BatchNormalization()(model) # å°è¯•å»æ‰ã€‚
  output = Activation('softmax', name="softmax")(model)

  ## å¤šä»»åŠ¡ï¼Œé¢„æµ‹è¡Œä¸ºã€åœºæ™¯ã€åŠ¨ä½œå…¬ç”¨ä¸€ä¸ªç½‘ç»œï¼Œå¯ä»¥ç¼“è§£è¿‡æ‹Ÿåˆï¼Œå…¶ä¸­è¡Œä¸ºä½œä¸ºä¸»ä»»åŠ¡ï¼Œåœºæ™¯ã€åŠ¨ä½œä½œä¸ºè¾…åŠ©ä»»åŠ¡ã€‚ Hardå‚æ•°å…±äº«
  # model_behavior = Dense(n_classes)(model) 
  # model_behavior = BatchNormalization()(model_behavior) 
  # output_behavior = Activation('softmax', name="behavior_softmax")(model_behavior)

  # model_action = Dense(n_classes)(model) 
  # model_action = BatchNormalization()(model_action) 
  # output_action = Activation('softmax', name="action_softmax")(model_action)

  # model_scene = Dense(n_scene)(model) 
  # model_scene = BatchNormalization()(model_scene) 
  # output_scene = Activation('softmax', name="scene_softmax")(model_scene)

  # return Model(input, output_behavior), Model(input, output_action), Model(input, output_scene)
  return Model(input, output) 

kfold = StratifiedKFold(n_splits=20, shuffle=True, random_state=2020) 
for fold, (xx, yy) in enumerate(kfold.split(x, y)): 
  model = Net()
  model.compile(loss = 'CategoricalCrossentropy', optimizer = 'Nadam', metrics = ['acc']) # å‡½æ•°çš„è¿”å›å€¼ä¸ºacc_combo get_acc_combo()
  graph_path = models_path + 'merged_model.png'
#   plot_model(model, to_file = graph_path, show_shapes = True)  # ç»˜åˆ¶æ¨¡å‹å›¾
  plateau = ReduceLROnPlateau(monitor = 'val_acc', verbose = 0, mode = 'max', factor = 0.1, patience = 8) # 'val_acc_combo'
  early_stopping = EarlyStopping(monitor = 'val_acc', verbose = 0, mode = 'max', patience = 30) # é˜²æ­¢è¿‡æ‹Ÿåˆ
  checkpoint = ModelCheckpoint(models_path + f'fold{fold}.h5', monitor = 'val_acc', verbose = 0, mode = 'max', save_best_only = True)
  # å•é€šé“æ¨¡å‹
  history = model.fit(x[xx],
              y_[xx], 
              epochs=epochs, 
              batch_size=batch_size, 
              verbose=500, 
              shuffle=True,
              validation_data=(x[yy], y_[yy]), 
              callbacks=[early_stopping, plateau, checkpoint] ## æ³¨ï¼šcallbacksï¼šè¾“å…¥çš„æ˜¯listç±»å‹çš„æ•°æ®ã€‚æœ‰éªŒè¯é›†ï¼Œç”¨â€™val_accâ€™
             ) ## æ³¨ï¼šmodel.fitï¼ˆï¼‰ä¸è¿”å›Kerasæ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªHistoryå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«è®­ç»ƒçš„æŸå¤±å’Œåº¦é‡å€¼ã€‚

  valid[yy] += model.predict(x[yy], verbose=1, batch_size=batch_size)
  # model.save(models_path + "merged_dcl.h5")  # å­˜å‚¨æ¨¡å‹
  ## å¯¹äºæ¯ä¸€æŠ˜çš„éªŒè¯é›†éƒ½è®¡ç®—å¾—åˆ†
  val = model.predict(x[yy], verbose=1, batch_size=batch_size) 

  val_labels = np.argmax(val, axis=1) # æ¯ä¸€è¡Œæœ€å¤§æ¦‚ç‡çš„ç´¢å¼•ï¼Œå³è¡Œä¸ºid
  val_score = sum(acc_combo(y_true, y_pred, 'behavior') for y_true, y_pred in zip(y[yy], val_labels)) / val_labels.shape[0]
  ### æ”¹ï¼šy --> new_y
  print('å®˜æ–¹å¾—åˆ†ï¼š', round(val_score, 5)) # ä¿ç•™å°æ•°ç‚¹åäº”ä½
  print('å‡†ç¡®ç‡å¾—åˆ†ï¼š', round(accuracy_score(y[yy].values, val_labels), 5)) 
  ### æ”¹ï¼šy --> new_y
  proba_tA += model.predict(t, verbose=1, batch_size=batch_size) / 20. # 5æŠ˜é¢„æµ‹æµ‹è¯•é›†ç„¶åå–å‡å€¼ (7500, 19)

np.save('./npy_file/lstm_valid.npy', valid)
np.save('./npy_file/lstm_test.npy', proba_tA)